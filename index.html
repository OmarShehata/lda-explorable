<!DOCTYPE html>
<html>
<head>
	<title>A Geometric Intuition for LDA</title>
	<link rel = "stylesheet" type = "text/css" href = "style.css" />
	<script src="lib/two.min.js"></script>
    <script src="lib/Tween.js"></script>
    <script src="lib/papaparse.min.js"></script>
    <script src="lib/numbers.min.js"></script>
    <script src="lib/numeric-1.2.6.min.js"></script>
    <script src="src/LDA.js"></script>
    <script src="src/main.js"></script>
</head>
<body>
<div class="article">
	<h1 class="centered-text">
		A Geometric Intuition for Linear Discriminant Analysis
	</h1>
	<p class="centered-text faded-text subtitle">
		Omar Shehata – St. Olaf College – 2018
	</p>
	<p class="centered-text faded-text">
		<i>(<span id="minutes_read"></span> min read)</i>
	</p>
	<!-- 
	Sources: https://sebastianraschka.com/Articles/2014_python_lda.html#%E2%80%93-bit-by-bit
	-->
	<p>Linear Discriminant Analysis is a useful technique in machine learning for <b>classification</b> and <b>dimensionality reduction</b> (also used in facial recognition? https://link.springer.com/article/10.1007%2Fs10115-006-0013-y ).</p>
	<p>The idea that you can take a rich 10 dimensional space and reduce it to 3 dimensions, while keeping most of the information intact, has always seemed a bit like magic to me. It's often used as a preprocessing step since a lot of algorithms perform better on a smaller number of dimensions. And of course, visualizing and reasoning about a 3 dimensional space is mucher easier than a higher dimensional one.</p> 
	<p>I like to think of Linear Discriminant Analysis as a mathematical tool that allows you to peer into spaces that cannot be seen. It's like piercing the veil of the unknowable.</p>
	<p>This article will explore this statistical technique with a geometric approach. "Add geometry quote here". No background in machine learning is expected. </p>
	<!-- 
	- Add quote by William K CLifford abotu how geometric is the gate to science and so easy a child can go through it. 
	-->
	<hr>
	<h2>The Problem</h2>
	<p>You're given high dimensional data and you're trying to reduce it. This need not be anything esoteric. It could be data about houses on the market with price, age, size, distance to public transport and number of rooms. That's already 5 dimensions – too many to visualize all at once.</p>
	<p>Like any good mathematician, we can start with a simple approach and explore its consequences. What happens if we just drop one of the dimensions?</p>
	<p>Let's see how this would work in a simple case. Here is some made up data about houses that's only in 2 dimensions. Blue dots could be houses that were sold and red ones are still looking for a buyer.</p>
	<div id="projection-2d" class="figure" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);"></div>
	<p>The graph on the left is the original data, and the line on the right is what it would look like after we reduce it 1 dimension.</p>
	<p>This is certainly one way to reduce it, but remember that our goal is to reduce while keeping as much information as we can intact. You can easily tell when looking at the 2D view that there is a pattern  to the data. This information is completely lost in the 1D view.</p>
	
	<p>Can we do better? We dropped the Y axis above, so perhaps we could drop the X axis instead? You can see below that that's not much better. But notice that these aren't the only options. Geometrically, there are an infinite number of 1 dimensional lines that we could project down onto! Click and drag in the 2D graph below to rotate the projection line. </p> 
	<div id="projection-2d-2" class="figure" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);"></div>
	<p>Can you see how in some projections it's much easier to see the separation than in others? In fact, there is one particular angle, at <span id="best-2d-projection" class="projection-link">y = 0</span> degrees that produces the best possible separation for this data. If we were to then apply some classification algorithm, it would preform just as well on the reduced data as on the original, so we can throw away the high dimensional data and just work with the low dimensional!</p>
	<p>The axis that remains is neither the original X nor the Y. It has no meaningful units on its own, but you can see it's not hard to reverse (example of this?).</p>
	<h2>In 3D</h2>
	<p>Let's try it with one more dimension up. Let's see if we can reduce 3D data to 2D. Use WS, AD, and QE to rotate.</p>
	<p><i>Side by side diagram of 3D with rotating plane and 2D. Should already be rotating.</i></p>
	<p>It's already much harder. There's 3 degrees of motion for a plane in 3D as opposed to 1 for a line in 2D. The optimal here is FIND AND LINK. We need an algorithm so we can find the precise optimal, but also for situations where we can't even visualize the higher dimension. </p>
	<p>Here is 4 dimensional data projected down to two. There are 6 degrees of motion. What we want is just like above, to find the projection that gives us the best separation in a lower dimension.</p>
	<p><i>One 2D view, with 6 angles on top, of a rotating plane that projects 4D points into 2D.</i></p>
	<p>This one is much harder to do intuitively by hand. When our intuition fails, we look to mathematics to guide us.</p>
	<h2>The Algorithm</h2>
	<p>The exciting part of mathematics, for me, is figuring out how to take a real world problem like that and phrase it in a way that our mathematical tools can tackle.</p>
	<p>The first step is to recognize this as an optimization problem. We know it's an optimization problem because there exists a set of options, all one dimensional lines in the 2D case, and we're trying to find the best one.</p>
	<p>But how do we know which one is the "best" ? We know intuitively because it gives us the best separation. So far we've been doing this visually. We need to precisely articulate this definition to solve it as an optimization problem. </p>
	<p>This is really the fun part of mathematics. The rest is just details, implementation the solution. What sort of metric can you come up with that would give you a high number for these cases:</p>
	<p><i>Show 2D and 1D projections with good separation</i></p>
	<p>Compared to these cases</p>
	<p><i>Show 2D and 1D projections with BAD separation</i></p>
	<p>As a simple example of a metric, you could say it's the absolute distance between the means of the two classes. That would certainly work above, but not when the data is so spread out:</p>
	<p><i>Show 2D and 1D projections with very high variance</i></p>
	<p>Pretend like it's the 1930's and this hasn't been solved yet. Give it some thought, what would you come up with?</p>
	<hr>
	<p>If you came up with a solution that took into account both the means and the variance (spread), then give yourself a pat on the back. This is exactly what Ronald Fisher came up with 1936.</p>
	<!--  Add something about Fisher's personality? How not to be wrong made it fun -->
	<p>The exact formula he came up with is:</p>
	<p>(U_1 - U_2)^2 / S_1 + S_2</p>
	<p>Where S_i is the scatter. It's a measure of how spread out the data is. So we want to maximize this function, which means we want the biggest distance between the means, but also the smallest scatter.</p>
	<p>To see how it works, it always helps to see an example. Here's the function computed for a particular example. Move it to see how it changes for different projections.</p>
	<p><i>Put formula next to 2D example projected down.</i></p>
	<p>You can see how this (LINK) produces bad separation, and is thus a low number, an this (LINK) produces good separation and is thus a higher number.</p>
	<p>Notice that both cases there actually have the same mean distances to each other, which is why it's not sufficient just to use that as a metric. </p>
	<p>Now this isn't the only way to do it, and I encourage you to test out any different methods you came up with. That is the joy of maths. If you come up with something, publish it! </p>
	<h2>Implementation Details</h2>
	<p>Actually optimizing this is a matter of a little bit of calculus and a little bit of linear algebra. I won't go into the derivation here but (http://courses.cs.tamu.edu/rgutier/csce666_f13/l10.pdf) is a good resource. It does generalize to more than 2.</p>
	<p>Most statistical packages will have an implementation. Let's see if using LDA will give us the same answer to what the best projection is for the 2D case that we found by hand.</p>
	<p>In Python, you can do it like this</p>
	<p><i>Python example with data hardcoded in.</i></p>
	<p>You can run this online on Google's Colaboratory (https://colab.research.google.com). The result you'll get is X. Here's how to use it to find the projection. Does that seem true if you try it in the diagram?</p>
	<p>Now is the real power. We can just as easily apply this onto the 4th dimensional example. Show some code, show the result. </p>
	<p>And there it is. A beautiful hidden structure, buried in esoteric 4 dimensional space, previous unseeable, laid bare before our eyes.</p>
	<p>Those are the leaps we make with mathematics. That is the power and joy.</p>
	<hr>
	<div id="credit">
		<p>Thanks to Professor Matt Richey for his amazing and inspiring lectures in the Algorithms for Decision Making class.</p>
		<p>The source code for this page and all interactive diagrams is available on <a href="https://github.com/OmarShehata/lda-explorable">GitHub</a> and is public domain. I've set it up so it would be really easy to replace the sample data I have and re-run the diagrams with your own data. In fact, you can just drag and drop any .csv file with the right number of columns onto a diagram and it'll use the new column names and data. Hopefully you'll find this a useful tool in the classroom. </p>
	</div>
</div>
<script type="text/javascript">
	// Compute how many minutes to read 
	var wordCount = document.querySelector(".article").textContent.split(" ").length;
	var minutes = Math.round(wordCount / 200);
	document.querySelector("#minutes_read").innerHTML = String(minutes);
</script>
</body>
</html>