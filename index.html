<!DOCTYPE html>
<html>
<head>
	<title>A Geometric Intuition for LDA</title>
	<link rel = "stylesheet" type = "text/css" href = "style.css" />
</head>
<body>
<div class="article">
	<h1 class="centered-text">
		A Geometric Intuition for Linear Discriminant Analysis
	</h1>
	<p class="centered-text faded-text subtitle">
		Omar Shehata – St. Olaf College – 2018
	</p>
	<p class="centered-text faded-text">
		<i>(<span id="minutes_read"></span> min read)</i>
	</p>
	<!-- 
	Sources: https://sebastianraschka.com/Articles/2014_python_lda.html#%E2%80%93-bit-by-bit
	-->
	<p>Linear Discriminant Analysis is a useful tool in machine learning for <b>classification</b> and <b>dimensionality reduction</b> (also used in facial recognition? https://link.springer.com/article/10.1007%2Fs10115-006-0013-y ).</p>
	<p>The idea that you can take a 10 dimensional space and reduce it to 3 dimensions, while keeping most of the information intact, has always seemed a bit like magic to me. It's a mathematical technique that allows you to peer into spaces that cannot be seen. It allows you visualize, work with and analyze these confounding high dimensional spaces. It's like piercing the veil of the unknowable.</p>
	<p>This article will explore this statistical technique with a focus on its geometric nature. No background in machine learning is needed. </p>
	<!-- 
	- Add quote by William K CLifford abotu how geometric is the gate to science and so easy a child can go through it. 
	-->
	<hr>
	<h2>The Problem</h2>
	<p>High dimensional space is hard to work with, just computationally but also visually/intuitively. So we want to simplify it.</p>
	<p>The problem is, how do you do that? We could just drop but then we're probably losing important information. Let's try a very simple case. Here you've got 2D data, and you want to reduce it 1 dimension.</p>
	<p><i>Side by side of blue and red dots, projected down so they're completely overlapping.</i></p>
	<p>Under this projection, the 1 dimensional view contains less information than the higher dimensional view. We are essentially dropping the Y axis. Another way to think about it is if you just looked at the 1D view you'd think there was no pattern to the data, no way to predict a dot's color based on its weight. But this is not true and the pattern is obvious when you look at the higher dimensional view.</p>
	<p>But maybe there's a better way to project data. We projected down onto a horizontal line, but there's an infinite number of them we can try! Click or drag to rotate the projection line.</p>
	<p><i>Side by side of same diagram.</i></p>
	<p>Can you see how in some projections it's much easier to see the separation than in others? In fact, there is one particular angle, at [USE LDA TO FIND THIS, make it a link that sets the diagram and scrolls up to it] degrees that produces the best possible separation for this data. If we were to then apply some classification algorithm, it would preform just as well on the reduced data as on the original, so we can throw away the high dimensional data and just work with the low dimensional!</p>
	<p>The axis that remains is neither the original X nor the Y. It has no meaningful units on its own, but you can see it's not hard to reverse (example of this?).</p>
	<h2>In 3D</h2>
	<p>Let's try it with one more dimension up. Let's see if we can reduce 3D data to 2D. Use WS, AD, and QE to rotate.</p>
	<p><i>Side by side diagram of 3D with rotating plane and 2D. Should already be rotating.</i></p>
	<p>It's already much harder. There's 3 degrees of motion for a plane in 3D as opposed to 1 for a line in 2D. The optimal here is (FIND AND LINK). We need an algorithm so we can find the precise optimal, but also for situations where we can't even visualize the higher dimension. </p>
	<p>Here is 4 dimensional data projected down to two. There are 6 degrees of motion. What we want is just like above, to find the projection that gives us the best separation in a lower dimension.</p>
	<p><i>One 2D view, with 6 angles on top, of a rotating plane that projects 4D points into 2D.</i></p>
	<p>This one is much harder to do intuitively by hand. When our intuition fails, we look to mathematics to guide us.</p>
	<h2>The Algorithm</h2>
	<p>The exciting part of mathematics, for me, is figuring out how to take a real world problem like that and phrase it in a way that our mathematical tools can tackle.</p>
	<p>The first step is to recognize this as an optimization problem. We know it's an optimization problem because there exists a set of options, all one dimensional lines in the 2D case, and we're trying to find the best one.</p>
	<p>But how do we know which one is the "best" ? We know intuitively because it gives us the best separation. So far we've been doing this visually. We need to precisely articulate this definition to solve it as an optimization problem. </p>
	<p>This is really the fun part of mathematics. The rest is just details, implementation the solution. What sort of metric can you come up with that would give you a high number for these cases:</p>
	<p><i>Show 2D and 1D projections with good separation</i></p>
	<p>Compared to these cases</p>
	<p><i>Show 2D and 1D projections with BAD separation</i></p>
	<p>As a simple example of a metric, you could say it's the absolute distance between the means of the two classes. That would certainly work above, but not when the data is so spread out:</p>
	<p><i>Show 2D and 1D projections with very high variance</i></p>
	<p>Pretend like it's the 1930's and this hasn't been solved yet. Give it some thought, what would you come up with?</p>
	<hr>
	<p>If you came up with a solution that took into account both the means and the variance (spread), then give yourself a pat on the back. This is exactly what Ronald Fisher came up with 1936.</p>
	<!--  Add something about Fisher's personality? How not to be wrong made it fun -->
	<p>The exact formula he came up with is:</p>
	<p>(U_1 - U_2)^2 / S_1 + S_2</p>
	<p>Where S_i is the scatter. It's a measure of how spread out the data is. So we want to maximize this function, which means we want the biggest distance between the means, but also the smallest scatter.</p>
	<p>To see how it works, it always helps to see an example. Here's the function computed for a particular example. Move it to see how it changes for different projections.</p>
	<p><i>Put formula next to 2D example projected down.</i></p>
	<p>You can see how this (LINK) produces bad separation, and is thus a low number, an this (LINK) produces good separation and is thus a higher number.</p>
	<p>Notice that both cases there actually have the same mean distances to each other, which is why it's not sufficient just to use that as a metric. </p>
	<p>Now this isn't the only way to do it, and I encourage you to test out any different methods you came up with. That is the joy of maths. If you come up with something, publish it! </p>
	<h2>Implementation Details</h2>
	<p>Actually optimizing this is a matter of a little bit of calculus and a little bit of linear algebra. I won't go into the derivation here but (http://courses.cs.tamu.edu/rgutier/csce666_f13/l10.pdf) is a good resource.</p>
	<p>Most statistical packages will have an implementation. Let's see if using LDA will give us the same answer to what the best projection is for the 2D case that we found by hand.</p>
	<p>In Python, you can do it like this</p>
	<p><i>Python example with data hardcoded in.</i></p>
	<p>You can run this online on Google's Colaboratory (https://colab.research.google.com). The result you'll get is X. Here's how to use it to find the projection. Does that seem true if you try it in the diagram?</p>
	<p>Now is the real power. We can just as easily apply this onto the 4th dimensional example. Show some code, show the result. </p>
	<p>And there it is. A beautiful hidden structure, buried in esoteric 4 dimensional space, previous unseeable, laid bare before our eyes.</p>
	<p>Those are the leaps we make with mathematics. That is the power and joy.</p>
	<hr>
	<div id="credit">
		<p>Thanks to Professor Matt Richey for his amazing and inspiring lectures in the Algorithms for Decision Making class.</p>
		<p>The source code for this page and all interactive diagrams is available on <a href="https://github.com/OmarShehata/lda-explorable">GitHub</a> and is public domain. I've set it up so it would be really easy to replace the sample data I have and re-run the diagrams with your own data. In fact, you can just drag and drop any .csv file with the right number of columns onto a diagram and it'll use the new column names and data. Hopefully you'll find this a useful tool in the classroom. </p>
	</div>
</div>
<script type="text/javascript">
	// Compute how many minutes to read 
	var wordCount = document.querySelector(".article").textContent.split(" ").length;
	var minutes = Math.round(wordCount / 200);
	document.querySelector("#minutes_read").innerHTML = String(minutes);
</script>
</body>
</html>