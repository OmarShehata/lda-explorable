<!DOCTYPE html>
<html>
<head>
	<title>A Geometric Intuition for LDA</title>
	<link rel = "stylesheet" type = "text/css" href = "style.css" />
	<script src="lib/two.min.js"></script>
    <script src="lib/Tween.js"></script>
    <script src="lib/papaparse.min.js"></script>
    <script src="lib/versor.js"></script>
    <script src="lib/numbers.min.js"></script>
    <script src="lib/numeric-1.2.6.min.js"></script>
    <script src="lib/threejs/three.js"></script>
    <script src="lib/threejs/OrbitControls.js"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    <script src="src/ThreePlotting.js"></script>
    <script src="src/LDA.js"></script>
    <script src="src/MathLibrary.js"></script>
    <script src="src/main.js"></script>
    <meta charset="UTF-8"> 
</head>
<body>
<div class="article">
	<h1 class="centered-text">
		A Geometric Intuition for Linear Discriminant Analysis
	</h1>
	<p class="centered-text faded-text subtitle">
		Omar Shehata &mdash; St. Olaf College &mdash; 2018
	</p>
	<p class="centered-text faded-text">
		<i>(<span id="minutes_read"></span> min read)</i>
	</p>
	<!-- 
	Sources: https://sebastianraschka.com/Articles/2014_python_lda.html#%E2%80%93-bit-by-bit
	-->
	<p>Linear Discriminant Analysis, or LDA, is a useful technique in machine learning for <b>classification</b> and <b>dimensionality reduction</b>. It's often used as a preprocessing step since a lot of algorithms perform better on a smaller number of dimensions. </p>
	<p>The idea that you can take a rich 10 dimensional space and reduce it to 3 dimensions, while keeping most of the information intact, has always seemed a bit like magic to me. I like to think of LDA as a mathematical tool that allows you to peer into spaces that cannot be seen. It's like piercing the veil of the unknowable.</p> 
	<p>
		In this article, I'd like to explore the fascinating geometric side of this popular statistical technique. My hope is that this focus on geometric intuition makes these ideas accessible to many more people.  
	</p>
	<blockquote>
		For geometry is the gate of science, and the gate is so low and small that one can only enter it as a child.
	</blockquote>
	<cite>&mdash; William K. Clifford</cite>
	<hr>
	<h2>The Premise</h2>
	<p>You're given high dimensional data and you're trying to reduce it. This need not be anything esoteric. It could be data about houses on the market with price, age, size, distance to public transport and number of rooms. That's already 5 dimensions &mdash; too many to visualize all at once.</p>
	<p>Like any good mathematician, you can start with a simple approach and explore its consequences. What happens if you just drop one of the dimensions?</p>
	<p>Let's see what this looks like in a simple case. Here is some made up data about houses that's only in 2 dimensions. Blue dots could be houses that were sold and red ones are still looking for a buyer.</p>
	<div id="projection-2d" class="figure" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);" data-initial-line='{"x":1, "y":0.001}'></div>
	<p>The graph on the left is the original data, and the line on the right is what it would look like after we drop it 1 dimension by just ignoring the Y axis.</p>
	<p>We've succesfully reduced our data, but remember that our goal is to reduce it without losing information. In this case, you can easily tell that there is a pattern to the data when looking at the 2D view. This pattern is completely lost in the 1D view.</p>	
	<p>Another way to think about this is to consider prediction. Given a new unclassified dot in the 2D view, you can easily guess whether it should be blue or red based on whether it's closer to the upper right or bottom left cluster. This would be much harder if you were given just the 1D view. In other words, our prediction algorithms would perform <i>worse</i> on the reduced data. </p>
	<p>We've sacrificed accuracy for simplicity, but maybe there's a better way to reduce it. Perhaps we could drop the X axis instead?</p>
	<p>Click and drag in the 2D graph below to rotate the projection line. </p> 
	<div id="projection-2d-2" class="figure" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);" data-initial-line='{"x":1, "y":0.001}'></div>
	<p>It seems that <span class="projection-link" data-line='{"x":0.001, "y": 1}' data-figure="projection-2d-2">dropping the X axis</span> isn't any better, but these aren't the only two choices. Geometrically, we have an infinite number of lines we can project on! </p>
	<p>Out of all these possible lines, the line <span class="projection-link" data-figure="projection-2d-2" data-line='{"best":true}'>y = 0</span> provides the best possible separation. This best line is what LDA allows us to find.</p>
	<p>The reduced data is only 1 dimensional, but it captures the important insight of the higher dimensional view. Think again about prediction. If we're looking at the 1D view, all we have to do to predict whether a new point should be blue or red is just see whether it's on the right or left side. </p>
	<p>This means that not only would our prediction algorithms perform just as good on the reduced data, it might even perform better! Strangely enough, by removing information, we've actually gained some insight. </p>
	<h2>Higher Dimensions</h2>
	<p>To build on this intuition, let's go one dimension up and look at an example that we can still visualize in its unreduced form.</p>
	<p>Here we've got 3D data, this time with 3 classes, and we want to reduce it to 2D. The diagram below shows one such projection. </p>
	<p>If you're using a keyboard, you can rotate the projection plane with W/S, A/D and Q/E.</p>
	<div id="container-3d" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);">
		<canvas id="projection-3d" class="figure"></canvas>
		<div id="projection-3d-2d" class="figure"></div>
		<div style="clear:both"></div>
	</div>
	<p>Again, there's an obvious pattern to the data in its original form that's lost in the reduced view. Finding the best projection plane by hand that retains this pattern is a harder problem now because there's three degrees of motion for a plane in 3D compared to one for a line in 2D.</p>
	<p>For this dataset, this best plane is <span class="projection-link" data-figure="projection-3d" data-line='{"best":true}'>x + y + z = 0</span>. Again, this best projection is what LDA allows us to find.</p>
	<p>Now here's the real test. What if we had 4D data that we couldn't even visualize? Below is what it looks projected down to 2D, (you can think of it as first projecting into 3D and then to 2D).</p>
	<div id="container-4d" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);">
		<div id="angles-4d" class="figure figure-left">
			<ul>
				<li>&alpha;<sub>XY</sub> &nbsp;= <input id="angle-xy" value="0"></input></li>
				<li>&alpha;<sub>XZ</sub> &nbsp;= <input id="angle-xz" value="0"></input></li>
				<li>&alpha;<sub>YZ</sub> &nbsp;= <input id="angle-yz" value="0"></input></li>
				<li>&alpha;<sub>XW</sub> = <input id="angle-xw" value="0"></input></li>
				<!-- 
					The reason rotating along the YW has no visible effect is because
					it's the same as rotating a line in 3D. One of the 3 rotations will be 
					perpendicular to its axis, which doesn't change the line.
				-->
				<li style="display:none;">&alpha;<sub>YW</sub> = <input id="angle-yw" value="0"></input></li>
				<li>&alpha;<sub>ZW</sub> = <input id="angle-zw" value="0"></input></li>
			</ul>
		</div>
		<div id="projection-4d" class="figure figure-right"></div>
		<div style="clear:both"></div>
	</div>
	<p>It looks as if there's no pattern here, and no hope of creating a good prediction algorithm. </p>
	<h2>Devising an Algorithm</h2>
	<p>The exciting part of mathematics, for me, is figuring out how to take a real world problem like that and phrase it in a way that our mathematical tools can tackle.</p>
	<p>The first step is to recognize this as an optimization problem. We know it's an optimization problem because there exists a set of options, all one dimensional lines in the 2D case, and we're trying to find the best one.</p>
	<p>But how do we know which one is the "best" ? We know intuitively because it gives us the best separation. So far we've been doing this visually. We need to precisely articulate this definition to solve it as an optimization problem. </p>
	<p>This is really the fun part of mathematics. The rest is just details, implementation the solution. What sort of metric can you come up with that would give you a low number for this projection:</p>
	<div id="bad-separation" class="figure"></div>
	<p>Compared to this projection</p>
	<div id="good-separation" class="figure"></div>
	<p>As a simple example of a metric, you could say it's the absolute distance between the means of the two classes. That would certainly work above, but not when the data is so spread out:</p>
	<div id="high-variance" class="figure"></div>
	<p>Pretend like it's the 1930's and this hasn't been solved yet. Give it some thought, what would you come up with?</p>
	<hr>
	<p>If you came up with a solution that took into account both the means and the variance (spread), then give yourself a pat on the back. This is exactly what Ronald Fisher came up with 1936.</p>
	<!--  Add something about Fisher's personality? How not to be wrong made it fun -->
	<p>The exact formula he came up with is:</p>
	<p>$$\frac{(\mu_1 - \mu_2)^2}{S_1 + S_2}$$</p>
	<p>Where $S_i$ is the scatter and is defined as $S_i = \sum_{y\in w_i} (y - \mu_i)^2$. It's a measure of how spread out the data is. So we want to maximize this function, which means we want the biggest distance between the means, but also the smallest scatter.</p>
	<p>To see how it works, it always helps to see an example. Here's the function computed for a particular example. Move it to see how it changes for different projections.</p>
	<div id="container-formula" ondragover="dragOverHandler(event);" ondragleave="dragLeaveHandler(event);" ondrop="dropHandler(event);">
		<div id="fisher-formula" class="figure figure-left">
			<p>$$\frac{(\mu_1 - \mu_2)^2}{S_1 + S_2} = x$$</p>
		</div>
		<div id="projection-fisher" class="figure figure-right"></div>
		<div style="clear:both"></div>
	</div>
	<p>You can see how this (LINK) produces bad separation, and is thus a low number, and this (LINK) produces good separation and is thus a higher number.</p>
	<p>Notice that both cases there actually have the same mean distances to each other, which is why it's not sufficient just to use that as a metric. </p>
	<p>Now this isn't the only way to do it, and I encourage you to test out any different methods you came up with. That is the joy of maths. If you come up with something, publish it! </p>
	<h2>Implementation Details</h2>
	<p>Actually optimizing this is a matter of a little bit of calculus and a little bit of linear algebra. I won't go into the derivation here but (http://courses.cs.tamu.edu/rgutier/csce666_f13/l10.pdf) is a good resource. It does generalize to more than 2.</p>
	<p>Most statistical packages will have an implementation. Let's see if using LDA will give us the same answer to what the best projection is for the 2D case that we found by hand.</p>
	<p>In Python, you can do it like this</p>
	<p><i>Python example with data hardcoded in.</i></p>
	<p>You can run this online on Google's Colaboratory (https://colab.research.google.com). The result you'll get is X. Here's how to use it to find the projection. Does that seem true if you try it in the diagram?</p>
	<p>Now is the real power. We can just as easily apply this onto the 4th dimensional example. Show some code, show the result. </p>
	<p>And there it is. A beautiful hidden structure, buried in esoteric 4 dimensional space, previous unseeable, laid bare before our eyes.</p>
	<p>Those are the leaps we make with mathematics. That is the power and joy.</p>
	<!-- 
		Say something about what the "linear" in LDA is all about. 

		Consider a ring inside a ring. THere's no way to separate this. 
	-->
	<hr>
	<div id="credit">
		<p>Thanks to Professor Matt Richey for his amazing and inspiring lectures in the Algorithms for Decision Making class.</p>
		<p>The source code for this page and all interactive diagrams is available on <a href="https://github.com/OmarShehata/lda-explorable">GitHub</a> and is public domain. I've set it up so it would be really easy to replace the sample data I have and re-run the diagrams with your own data. In fact, you can just drag and drop any .csv file with the right number of columns onto a diagram and it'll use the new column names and data. Hopefully you'll find this a useful tool in the classroom. </p>
	</div>
</div>
<script type="text/javascript">
	// Compute how many minutes to read 
	var wordCount = document.querySelector(".article").textContent.split(" ").length;
	var minutes = Math.round(wordCount / 200);
	document.querySelector("#minutes_read").innerHTML = String(minutes);

	
</script>
</body>
</html>